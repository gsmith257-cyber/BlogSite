---
description: BIT 3434 Research Project - Group 54
---

# ⛏ Data Mining CVEs and Exploits

For our data mining project in BIT 3434 we needed to find a topic and we wanted something interesting and not readily available online. This is when we decided to pick data mining in cybersecurity in order to answer this primary question:

> Though there are many CVEs out there, and more coming out each day, how many of them are actually exploitable by most threat actors?

To get started with answering this question we need a lot of data. We started by gathering all the CVE data from the [NIST National Vulnerability Database (NVD)](https://nvd.nist.gov/vuln/data-feeds). Each year has its own JSON file, except for years before 2002 but they are included in the 2002 file.

Now that we had each CVE we needed a way to search for exploits. My first thought was that I could search Offensive Security's ExploitDB and GitHub for proof of concept (PoC) scripts. This turned out not to be effective though due to issues parsing and searching ExploitDB correctly and GitHub's rate limiting. In order to parse the well over 100,000 CVEs we would need to do it offline to get anything done in a reasonable amount of time.

This is when I found that [ExploitDB has a CSV file](https://gitlab.com/exploit-database/exploitdb/-/blob/main/files\_exploits.csv) of all the exploits it currently has and most have a tag for the CVE it exploits. This was a game changer for the amount of time that was needed to go through all this data. After doing some math I found I needed around 240 hourse of time to parse through all this data with my script. Not too bad, but there was a problem... I had this project due in 120 hours. I had to find a way to drastically improve the speed of my script.

In order to do this I realized that the library I was using to write new rows to the xlsx files was opening and reading the entire file and then writing and then writing it all back again with the new row. This was OK for the first 2000 or so rows but after that it got painstakingly slow. To fix this I made it so after 1000 rows written it will create a whole new xlsx file and start writing to that and it will use the naming format of '{year}\_data{number\_of\_files\_for\_this\_year}.xlsx". This worked perfectly and I got the hours down to around 120 needed. To speed this up I split the task between my laptop and PC, having each do half.

The script, data, and results I made/used for this research is all in [this GitHub repository](https://github.com/gsmith257-cyber/BIT3434CVE).

Now lets look at the results and some other interesting data found!

Before we look at the data we "mined" I wanted to showcase some valuable data I found on [cvedetails.com](https://www.cvedetails.com/cvss-score-charts.php?fromform=1\&vendor\_id=\&product\_id=\&startdate=1999-01-01\&enddate=2022-12-05).

<figure><img src=".gitbook/assets/cveDetails.PNG" alt=""><figcaption><p>CVEdetails.com report</p></figcaption></figure>

In these statistics you can see the overall averages for CVE scores, as well as the distribution, throughout the time CVEs have been recorded.

Also before I reveal the new data I want to share some statistics about my experience parsing this data:

* Total hours processing data: 128
* Computers Used: 2
* Rows written (before cleanup): 185,023
* CVEs Processed: 175,026



### Results

Now, to the new data. With our main question being "Though there are many CVEs out there, and more coming out each day, how many of them are actually exploitable by most threat actors?" We got this answer as a result:

```
Percentage of CVEs that have public exploits: 7.591%
```

Now this was shockingly low. I had expected there to be at least 20% of CVEs. This just goes to show that despite the massive amount of CVEs that are published each year most attackers are only exploiting 7.6% of them.

> ...despite the massive amount of CVEs that are published each year most attackers are only exploiting 7.6% of them.

Our next interesting find was that the severity of each CVE is trending upwards, which can be seen in this graph and trend line:

<figure><img src=".gitbook/assets/CVEscore.PNG" alt=""><figcaption><p>Data on CVE CVSS Score by year</p></figcaption></figure>

We also gathered how many exploits were posted by year and here is the graph for this data:

<figure><img src=".gitbook/assets/exploits.PNG" alt=""><figcaption><p>Exploits published by year on ExploitDB</p></figcaption></figure>

All this data/script/results can be found on [my GitHub here](https://github.com/gsmith257-cyber/BIT3434CVE). The final sheet with all the data is in the results folder and is named 'allData.xlsx'.

### Summary

So why was this research important? As we get flooded with new, amazing CVEs that can gain remote code execution (RCE) on all these different systems and services, it is important to take a step back and look at the reality of how much is actually used in common attacks. We provide this data with our research here and it can be used by researchers to look for their own trends or build on their own datasets.

It is also my firm belief that as we get more reliant on technology the more vulnerable we are to it breaking. While correlation is not causation it is notable that the severity of CVEs continues trending upwards.

### Sources:

{% embed url="https://nvd.nist.gov/" %}

{% embed url="https://www.exploit-db.com/" %}
